# LLM Configuration
LLM_PROVIDER=openai
LLM_MODEL=gpt-4-turbo-preview
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2000

# OpenAI Settings
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_ORGANIZATION=your_organization_id_optional

# Anthropic Settings (if using Anthropic)
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Local Model Settings (if using local models)
# LOCAL_MODEL_PATH=/path/to/local/model
# LOCAL_MODEL_DEVICE=cpu  # or cuda, mps

# Caching
LLM_ENABLE_CACHING=true
LLM_CACHE_TTL=3600
